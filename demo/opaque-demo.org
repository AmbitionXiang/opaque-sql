* Opaque demo
** Setup
cd ~/spark
source conf/spark-env.sh
build/sbt package
cd ~/opaque
bin/spark-shell --master local[1] --jars $HOME/opaque/target/scala-2.11/opaque_2.11-0.1.jar --conf "spark.driver.extraJavaOptions=-Xdebug -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8001"

** In Spark shell
*** Import 
import edu.berkeley.cs.rise.opaque.implicits._
edu.berkeley.cs.rise.opaque.Utils.initSQLContext(spark.sqlContext)

*** Data creation
val data = for (i <- 0 until 10) yield ("foo", i)
val rdd_data = spark.sparkContext.makeRDD(data, 1)

*** DataFrame creation

val words = spark.createDataFrame(rdd_data).toDF("word", "count")
val words_e = spark.createDataFrame(rdd_data).toDF("word", "count").encrypted
val words_o = spark.createDataFrame(rdd_data).toDF("word", "count").oblivious

*** Query execution

words.filter($"count" > lit(3)).collect
words_e.filter($"count" > lit(3)).collect
words_o.filter($"count" > lit(3)).collect

** In JDB
PS1="\[\033[01;38;5;210m\]\u@\h (attacker"'!'")\[\033[00m\] \D{%Y-%m-%d %H:%M:%S} \[\033[01;34m\]\w\[\033[00m\]\n\$ "

jdb -sourcepath $HOME/spark/sql/core/src/main/scala:$HOME/opaque/src/main/scala -attach 8000

*** For attacking insecure Spark
stop at org.apache.spark.sql.execution.FilterExec$$anonfun$12$$anonfun$apply$2:126
list
print row
cont
cont
cont
cont
print row
set r = false
clear org.apache.spark.sql.execution.FilterExec$$anonfun$12$$anonfun$apply$2:126

*** For attempting to attack Opaque
stop at edu.berkeley.cs.rise.opaque.execution.ObliviousFilterExec$$anonfun$executeBlocked$14:323
list
dump filtered
set filtered[0] = 123
cont
